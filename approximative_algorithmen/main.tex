\documentclass[ngerman]{scrartcl}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{hyperref}

\title{Approximationsalgorithmen \\SoSe 2019}
\author{Benedikt Lüken-Winkels}
\begin{document}

\maketitle
\tableofcontents
\newpage
\begin{abstract}

\end{abstract}

\section{1.Vorlesung}
Foliensatz 1
\subsection{Orga}
\begin{itemize}
  \item \textbf{Sprechstunde} Do, 13-14 Uhr
  \item \textbf{Vorlesung} Di, 12:15-13:45
  \item \textbf{Übung} Di, 8:15-9:45 Uhr (erster Termin 16.04.)
  \item \textbf{Prüfung} Mündl Prüfung
\end{itemize}

\subsection{Einführung}
\subsubsection{Motivation}
\begin{itemize}
  \item wenn P $ \neq $ NP, kan man keinen guten oder schnellen Algorithmus schreiben
  \item Zeigt man, dass ein Problem NP-schwer ist, kann kein schneller Algorithmus geschrieben werden  
\end{itemize}
$\Rightarrow$ Heuristische Verfahren (keine mathematische Garantie). Warum funktionieren die Heuristiken so gut?
Herangehensweisen
\begin{itemize}
  \item Greedy Verfahren
  \item Randomisierte Verfahren: finden der Lösung mit hoher Wahrscheinlichkeit
  \item Parametrisierte Verfahren: exakte Lösungen und Versuch, den exponentiellen Teil gering zu halten
  \item Näherungsverfahren: Heuristiken mit Leistungsgarantie
\end{itemize}
Klasse von Problemen die zur Betrachtung stehen. \\
\textbf{Quatrupel} $ (I_P, S_P, m_P , opt_P) $ zur Beschreibung eines Optimierungsproblems
\begin{itemize}
  \item $ I_P $: geeignete Instanz eines Problems, genauer: "geeignet binär-codierte formale Sprachen".
  \item $ S_P $: Bildet auf Menge der möglichen Lösungen ab
  \item $ m_P $: x Instanz und y eine Lösung. Abbildung auf Maßzahl
  \item $ opt_P $: Möglichst kleines Ergebnis oder möglichst großes
\end{itemize}
\begin{itemize}
  \item $ S_P^*: I_P \rightarrow $ Menge der bestmöglichen Lösungen
  \item $ m_P^* $ Wert oder Grenzwert einer bestmöglichen Lösung
  \item * bedeutet idR bestmöglich
\end{itemize}
$\Rightarrow$ \textbf{Ziel}: Leistungsgröße (Folie 15) ist 1, wenn Lösung optimal ist

\subsubsection{Beispiel: Knotenüberdeckung}
Möglichst wenige Knoten, um alle Kanten abzudecken
\begin{itemize}
  \item Zuordnung zu den Optimierungsparametern Folie 17
  \item Verschiedene Beobachtungen zur Optimierung
  \begin{itemize}
    \item Zwei Knoten im Dreieck gehören dazu
    \item Bei Knoten mit Grad 1 wird immer der Nachbar genommen
  \end{itemize}
  \item Auswählen eines Knotens bedeutet, dass diese Teile abgeschnitten werden
  \item $\Rightarrow$ Vereinfachung des Graphen, zB neue Grad 1 Knoten
\end{itemize}

\paragraph*{Greedyverfahren, GreedyVC (Folie 23)}
\begin{itemize}
  \item Änderung der Grade bei Durchführung	
  \item Problem: Implementierung der Kantenlöschung (Kopieren des Graphen bei jeder Iteration nötig?)
  \item Folie 24: Lösung insofern (inklusions-) minimal, als dass das Entfernen eines Knotens keine andere Lösung zulässt
\end{itemize}

\paragraph*{Suchbaumverfahren, Entscheidungsproblem (Folie 25)}
Liefert exakte Lösungen
\begin{itemize}
  \item Zusätzlicher Parameter $ k $ ("Budget")
  \item Zwei Abbruchskriterien:
  \begin{itemize}
    \item Alle Kanten abgedeckt
    \item Nicht alle Kanten abgedeckt, aber $ k=0 $
  \end{itemize}
  \item Suchbaum im worst-case ein vollständiger Binärbaum, \textbf{aber} höchsten $ 2^{k} $ Schritte im Baum, da die Tiefe durch $ k $ begrenzt ist
\end{itemize}
\paragraph*{Näherungsverfahren (Folie 30)}
Suchbaumverfahren ohne Fallunterscheidung. (Faktor 2-Approximations-Verfahren)
\begin{itemize}
  \item Bei jeder Kante muss einer der Knoten in die Überdeckung 
  \item Lokaler Fehler höchsten Faktor 2
  \item Zufall bei der Auswahl der Kanten kann zum Vorteil sein
\end{itemize} 

Näherung gibt Schranke für die minimale Lösung dadurch, dass Heuristik eine Faktor 2 Lösung zeigt.
$\Rightarrow$ (Folie 31) Lösung mit 22 Knoten zeigt eine optimale Lösung mit 11 Knoten

\subsubsection{Beispiel: MAXSAT (Folie 32)}
$ mP $ = Anzahl der Klauseln, die die Formel erfüllen
\paragraph*{Einfacher Ansatz}
\begin{itemize}
  \item Alles 0 und alles 1 setzen, dann das bessere Ergebnis zurückliefern
  \item $\Rightarrow$ liefert 2-Approximation
\end{itemize}

\subsubsection{Beispiel: Unabhängige Knotenmengen (Folie 34)}
Sehr schwer approximierbar

\subsubsection{Beispiel: Unabhängige Kantenmengen (Folie 35)}
Lösung in Polinomialzeit, um eine untere Schranke für die Knotenüberdeckung zu finden


\section{2. Vorlesung}
2.Foliensatz

\subsection{Definition Gewichtsreduktionsfunktion}
Eine Reduktion verringert die Gewichtsfunktion: $ \forall x \in X : 0 \leq \delta(x) \leq w(x) $ \\
Eine Reduktion ist \textbf{r-effektiv}, wenn $ \delta(X) \leq r \cdot  OPT(\delta) $
\subsection{Allgemeines (gewichtetes) Überdeckungsproblem}
\begin{itemize}
  \item Grundmenge $ X $
  \item Monotone Abbildung (Bewerung: 1 = Überdeckung oder 0) $ f: 2^X \rightarrow \{0,1\} $
  \item Gewichtsfunktion $ w \rightarrow \mathbb{R}^+ $ weist den Knoten ein Gewicht zu
  \item $ \Rightarrow $ Überdeckung mit kleinstmöglichem Gewicht
  \item Gewichtsreduktionsfunktion $ \delta $
  \item $ OPT(w) = w(C^*) $ $ C^* $ ist optimale Überdeckung
\end{itemize}
Einfachere Problemanalyse durch Zerlegung von Gewichtsfunktionen in Untergewichtsfunktionen

\subsection{Reduktion Bar-Yehuda, Even Folie 13}
\textbf{2-Approximation}, Reduktion für jede Kante $ \delta_e (v) $ wird angewandt auf jeden anliegenden Knoten
\begin{itemize}
  \item Wähle das Minimum der Knoten als Gewicht für die Kante
  \item Nehme eine Kante und ziehe das Gewicht der Kante von den Knoten ab $ \Rightarrow $ einer der Knoten hat Grad 0 und damit Teil einer Überdeckung
  \item Nächster Schritt $ w - \delta_e $, bedeutet, dass die Gewichtsfunktion verändert wird und eine neue Iteration beginnt
\end{itemize}

\subsection{Reduktion Clarkson Folie 22}
\textbf{2-Approximation}, Gewichtsreduktion über Knoten
\begin{itemize}
  \item $ \varepsilon (v) = \frac{w(v)}{d(v)}$
  \item Anliegende Knoten von v erhalten Gewicht $ \varepsilon(v) $
  \item $ \Rightarrow w - \delta_v $
\end{itemize} 

\subsection{Randomiesierte Verfahren}
\textbf{2-Approximation}, Gewichtsreduktion über Knoten
\begin{itemize}
  \item Zufallsalgorithmus gemäß r-effektiver Verteilung (nicht immer Faktor r, aber im Mittel erreicht)
  \item Implementierung der Intuition, dass großgradige Knoten interessant sind
  \item Bei ungewichteten Graphen:
  \begin{itemize}
    \item ($ w(v) = 1 $)
    \item Wahrscheinlichkeit einen Knoten zu wählen, $ \frac{d(v)}{2|V|} $ ($ 2|V| $, weil alle Kanten Doppelt abgezählt werden)
    \item Knoten mit großem Grad werden häufig, aber \textbf{nicht immer} in die Überdeckung aufgenommen
  \end{itemize}
\end{itemize}

\subsection{$\Delta$-Hitting-Set}
$\Delta$ = maximaler Grad der Kanten (Wieviele Knoten hängen an einer Kante). $ \Delta = 2 $ quasi Knotenüberdeckungsproblem
\paragraph{Sonderfälle}
\begin{itemize}
  \item leere Kante (keine Knoten) $ \Rightarrow $ keine Überdeckung möglich
  \item Kante mit nur einem Knoten $ \Rightarrow $ automatisch hinzufügen
\end{itemize}
\subsubsection{Beispiel "Smart Home"}
\paragraph{System}
\begin{itemize}
  \item Systembestandteile C
  \item Systembeschreibung SD (wie das System sein sollte)
  \item beobachtetes Systemverhalten OBS
\end{itemize}
Ist ein Widerspruch in der Annahme, dass das System fehlerfrei funktioniert, finde die größte Menge an Systembestandteilen, sodass der Widerspruch verschwindet. (Schneide den defekten Teil des Systems ab)

\subsubsection{Datenreduktion}
\begin{itemize}
  \item Kante f ist echte Teilmenge von Kante e $ \Rightarrow $ entferne e
  \item Kante e ist gleich Knoten v $ \Rightarrow $ Knoten ist in der Überdeckung
  \item Konten x hat ist nur in einer Kante mit Knoten y $ \Rightarrow $ entferne x
\end{itemize}

\section{3. Vorlesung}
\subsection{Problemvarianten}

\paragraph{$ P_C $ Konstruktionsproblem} Zur Instanz eines Problems soll eine bestmögliche Lösung und deren Wert geliefert werden. Häufig existieren mehrere beste Lösungen mit gleichem Wert.

\paragraph{$ P_E $ Auswertungsproblem} Zur Instanz eines Problems wird nur der Wert der bestmögl Lösung benötigt.

\paragraph{$ P_D $ Entscheidungsproblem} Ist der Wert einer optimalen Lösung größer $ k $ oder kleiner.

\subsection{Klassen}
\subsubsection*{NPO}
\begin{itemize}
  \item Finden der Lösung, Entscheidung, ob die Lösung gültig ist und Maßzahl/Messfunktion sind in Polinomialzeit lösbar.
  \item Größe der Lösung ist durch die Größe der Instanz gedeckelt
  \item Lösen des NP Problems in nicht deterministischer Weise (Raten)
\end{itemize}
\paragraph{Turing-Reduzierbarkeit} Aufteilung des Problems in $ P_1 $ und $ P_2 $. $ P_2 $ ist ein Unterprogramm, das bei der Lösung einer Probleminstanz von $ P_1 $  hilft.
Bei einem NPO-Problem: $ P_D =^P P_E \leq^P P_C $. Ist P NP-Hart, gilt $ P_C \leq^P P_D $
\paragraph{MAXCLIQUE} (Lösung als Konstruktionsproblem mit Hilfe des Auswertungsproblems) Durch den Wert des Auswertungsproblems ($ P_E $) kann die größte Clique gefunden werden. Die \textbf{Orakelfunktion} liefert die Größe der Lösung.  

\subsection*{MinSAT}
\begin{itemize}
  \item Boolsche Formel als Instanz
  \item Maß ergibt sich aus all den Klauseln, die true ergeben
  \item Erfüllte Klauseln möglich gering halten. 
\end{itemize}
\paragraph{Lösung} Vertex Cover kann in MinSAT überführt werden und umgekehrt. Lösung für das eine Problem ist eine Lösung für das Andere Porblem

\subsection{Grundtechniken}
\subsubsection{Greedyverfahren (Maximierung) Folie 29}
\begin{itemize}
  \item Maximale Lösung finden
  \item Leer Menge ist eine zulässige Lösung
  \item Kann der Lösungsmenge kein weiteres Element hinzugefügt werden, gib das die Menge aus
\end{itemize}
\paragraph*{Ruchsackproblem (NP-vollständig)}
\begin{itemize}
  \item Möglichst viele schwere Gegenstände/möglichst voller Rucksack (Profitfunktion maximal)
  \item Komplexität ergibt sich aus Kodierung der Zahlen (mit Unärkodierung liegt das Problem in P)
\end{itemize}
\textbf{Heuristiken}
\begin{itemize}
  \item Gegenstände nach Verhältnis von Gewicht und Profit geordnet. dann einräumen, solange es geht
  \item Gegenstände nach Profit absteigend geordnet
\end{itemize} 
$ \Rightarrow $ Kombination beider Heuristiken liefert ein gutes Ergebnis. Zunächst die eine, dann die Andere Heuristik anwenden. Danach wähle das bessere Ergebnis.\\
FOLIE 40 EINFÜGEN

\section{4. Vorlesung}
\subsection{Max Independent Set}
Liegt in NP weil der Algorithmus eine Lösung rät und die Lösung dann in Polynomialzeit auf Korrekthei überprüft werden kann
\subsubsection*{Greedy zur Lösung}
Beispiel Folie 6
\begin{enumerate}
  \item Nimm klein-gradige Knoten $ w \Rightarrow$ alle $ u $ fallen weg
  \item Nimm ein $ v \Rightarrow$ alle anderen $ v $ fallen weg (Clique)
  \item Ind Set = 2  $ \Rightarrow $ belibig schlechtes Ergebnis
\end{enumerate}
Das Verhältnis zwischen $ m^* $ und $ m_Gr $ lässt sich durch die (beschränkte) Dichte des Graphen abschätzen

\begin{itemize}
  \item Greedy Liefert bei Bäumen immer eine Optimallösung 
  \item Gleiche Schwierigkeit wie MaxClique
  \begin{itemize}
    \item Independent Set des Komplementärgraphen ist Lösung. 
    \item oder Justierung des Alogrithmus: Nimm höchstgradigen Knoten, dann entferne alle nicht-Nachbarn
  \end{itemize}
  \item Bei Knotenfärbung bildet jede Farbe ein Independent Set. 
\end{itemize}

\subsection*{Geschäftsreisenproblem}
\begin{itemize}
  \item gerichtete Abstandsfunktion
  \item LÖsungen sind Permutationen der Städte
  \item $ m $, die Summe der Abstände sind die Kosten/Länge der Tour
\end{itemize}
Greedy Algorithmus nimmt eine Stadt und wählt die nächste mit dem kleinsten Abstand und entfernt diese



\end{document}


