\documentclass[ngerman]{scrartcl}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{hyperref}

\title{Approximationsalgorithmen \\SoSe 2019}
\author{Benedikt Lüken-Winkels}
\begin{document}

\maketitle
\tableofcontents
\newpage
\begin{abstract}

\end{abstract}

\section{1.Vorlesung}
Foliensatz 1
\subsection{Orga}
\begin{itemize}
  \item \textbf{Sprechstunde} Do, 13-14 Uhr
  \item \textbf{Vorlesung} Di, 12:15-13:45
  \item \textbf{Übung} Di, 8:15-9:45 Uhr (erster Termin 16.04.)
  \item \textbf{Prüfung} Mündl Prüfung
\end{itemize}

\subsection{Einführung}
\subsubsection{Motivation}
\begin{itemize}
  \item wenn P $ \neq $ NP, kan man keinen guten oder schnellen Algorithmus schreiben
  \item Zeigt man, dass ein Problem NP-schwer ist, kann kein schneller Algorithmus geschrieben werden  
\end{itemize}
$\Rightarrow$ Heuristische Verfahren (keine mathematische Garantie). Warum funktionieren die Heuristiken so gut?
Herangehensweisen
\begin{itemize}
  \item Greedy Verfahren
  \item Randomisierte Verfahren: finden der Lösung mit hoher Wahrscheinlichkeit
  \item Parametrisierte Verfahren: exakte Lösungen und Versuch, den exponentiellen Teil gering zu halten
  \item Näherungsverfahren: Heuristiken mit Leistungsgarantie
\end{itemize}
Klasse von Problemen die zur Betrachtung stehen. \\
\textbf{Quatrupel} $ (I_\rho, S_\rho, m_\rho , opt_\rho) $ zur Beschreibung eines Optimierungsproblems
\begin{itemize}
  \item $ I_\rho $: geeignete Instanz eines Problems, genauer: "geeignet binär-codierte formale Sprachen".
  \item $ S_\rho $: Bildet auf Menge der möglichen Lösungen ab
  \item $ m_\rho $: x Instanz und y eine Lösung. Abbildung auf Maßzahl
  \item $ opt_\rho $: Möglichst kleines Ergebnis oder möglichst großes
\end{itemize}
\begin{itemize}
  \item $ S_\rho^*: I_\rho \rightarrow $ Menge der bestmöglichen Lösungen
  \item $ m_\rho^* $ Wert oder Grenzwert einer bestmöglichen Lösung
  \item * bedeutet idR bestmöglich
\end{itemize}
$\Rightarrow$ \textbf{Ziel}: Leistungsgröße (Folie 15) ist 1, wenn Lösung optimal ist

\subsubsection{Beispiel: Knotenüberdeckung}
Möglichst wenige Knoten, um alle Kanten abzudecken
\begin{itemize}
  \item Zuordnung zu den Optimierungsparametern Folie 17
  \item Verschiedene Beobachtungen zur Optimierung
  \begin{itemize}
    \item Zwei Knoten im Dreieck gehören dazu
    \item Bei Knoten mit Grad 1 wird immer der Nachbar genommen
    \item 
  \end{itemize}
  \item Auswählen eines Knotens bedeutet, dass diese Teile abgeschnitten werden
  \item $\Rightarrow$ Vereinfachung des Graphen, zB neue Grad 1 Knoten
\end{itemize}

\paragraph*{Greedyverfahren, GreedyVC (Folie 23)}
\begin{itemize}
  \item Änderung der Grade bei Durchführung	
  \item Problem: Implementierung der Kantenlöschung (Kopieren des Graphen bei jeder Iteration nötig?)
  \item Folie 24: Lösung insofern (inklusions-) minimal, als dass das Entfernen eines Knotens keine andere Lösung zulässt
\end{itemize}

\paragraph*{Suchbaumverfahren, Entscheidungsproblem (Folie 25)}
Liefert exakte Lösungen
\begin{itemize}
  \item Zusätzlicher Parameter $ k $ ("Budget")
  \item Zwei Abbruchskriterien:
  \begin{itemize}
    \item Alle Kanten abgedeckt
    \item Nicht alle Kanten abgedeckt, aber $ k=0 $
  \end{itemize}
  \item Suchbaum im worst-case ein vollständiger Binärbaum, \textbf{aber} höchsten $ 2^{k} $ Schritte im Baum, da die Tiefe durch $ k $ begrenzt ist
\end{itemize}
\paragraph*{Näherungsverfahren (Folie 30)}
Suchbaumverfahren ohne Fallunterscheidung. (Faktor 2-Approximations-Verfahren)
\begin{itemize}
  \item Bei jeder Kante muss einer der Knoten in die Überdeckung 
  \item Lokaler Fehler höchsten Faktor 2
  \item Zufall bei der Auswahl der Kanten kann zum Vorteil sein
\end{itemize} 

Näherung gibt Schranke für die minimale Lösung dadurch, dass Heuristik eine Faktor 2 Lösung zeigt.
$\Rightarrow$ (Folie 31) Lösung mit 22 Knoten zeigt eine optimale Lösung mit 11 Knoten

\subsubsection{Beispiel: MAXSAT (Folie 32)}
$ m\rho $ = Anzahl der Klauseln, die die Formel erfüllen
\paragraph*{Einfacher Ansatz}
\begin{itemize}
  \item Alles 0 und alles 1 setzen, dann das bessere Ergebnis zurückliefern
  \item $\Rightarrow$ liefert 2-Approximation
\end{itemize}

\subsubsection{Beispiel: Unabhängige Knotenmengen (Folie 34)}
Sehr schwer approximierbar

\subsubsection{Beispiel: Unabhängige Kantenmengen (Folie 35)}
Lösung in Polinomialzeit, um eine untere Schranke für die Knotenüberdeckung zu finden


\section{2. Vorlesung}
2.Foliensatz

\subsection{Definition Gewichtsreduktionsfunktion}
Eine Reduktion verringert die Gewichtsfunktion: $ \forall x \in X : 0 \leq \delta(x) \leq w(x) $ \\
Eine Reduktion ist \textbf{r-effektiv}, wenn $ \delta(X) \leq r \cdot  OPT(\delta) $
\subsection{Allgemeines (gewichtetes) Überdeckungsproblem}
\begin{itemize}
  \item Grundmenge $ X $
  \item Monotone Abbildung (Bewerung: 1 = Überdeckung oder 0) $ f: 2^X \rightarrow \{0,1\} $
  \item Gewichtsfunktion $ w \rightarrow \mathbb{R}^+ $ weist den Knoten ein Gewicht zu
  \item $ \Rightarrow $ Überdeckung mit kleinstmöglichem Gewicht
  \item Gewichtsreduktionsfunktion $ \delta $
  \item $ OPT(w) = w(C^*) $ $ C^* $ ist optimale Überdeckung
\end{itemize}
Einfachere Problemanalyse durch Zerlegung von Gewichtsfunktionen in Untergewichtsfunktionen

\subsection{Reduktion Bar-Yehuda, Even Folie 13}
\textbf{2-Approximation}, Reduktion für jede Kante $ \delta_e (v) $ wird angewandt auf jeden anliegenden Knoten
\begin{itemize}
  \item Wähle das Minimum der Knoten als Gewicht für die Kante
  \item Nehme eine Kante und ziehe das Gewicht der Kante von den Knoten ab $ \Rightarrow $ einer der Knoten hat Grad 0 und damit Teil einer Überdeckung
  \item Nächster Schritt $ w - \delta_e $, bedeutet, dass die Gewichtsfunktion verändert wird und eine neue Iteration beginnt
\end{itemize}

\subsection{Reduktion Clarkson Folie 22}
\textbf{2-Approximation}, Gewichtsreduktion über Knoten
\begin{itemize}
  \item $ \varepsilon (v) = \frac{w(v)}{d(v)}$
  \item Anliegende Knoten von v erhalten Gewicht $ \varepsilon(v) $
  \item $ \Rightarrow w - \delta_v $
\end{itemize} 

\subsection{Randomiesierte Verfahren}
\textbf{2-Approximation}, Gewichtsreduktion über Knoten
\begin{itemize}
  \item Zufallsalgorithmus gemäß r-effektiver Verteilung (nicht immer Faktor r, aber im Mittel erreicht)
  \item Implementierung der Intuition, dass großgradige Knoten interessant sind
  \item Bei ungewichteten Graphen:
  \begin{itemize}
    \item ($ w(v) = 1 $)
    \item Wahrscheinlichkeit einen Knoten zu wählen, $ \frac{d(v)}{2|V|} $ ($ 2|V| $, weil alle Kanten Doppelt abgezählt werden)
    \item Knoten mit großem Grad werden häufig, aber \textbf{nicht immer} in die Überdeckung aufgenommen
  \end{itemize}
\end{itemize}

\subsection{$\Delta$-Hitting-Set}
$\Delta$ = maximaler Grad der Kanten (Wieviele Knoten hängen an einer Kante). $ \Delta = 2 $ quasi Knotenüberdeckungsproblem
\paragraph{Sonderfälle}
\begin{itemize}
  \item leere Kante (keine Knoten) $ \Rightarrow $ keine Überdeckung möglich
  \item Kante mit nur einem Knoten $ \Rightarrow $ automatisch hinzufügen
\end{itemize}
\subsubsection{Beispiel "Smart Home"}
\paragraph{System}
\begin{itemize}
  \item Systembestandteile C
  \item Systembeschreibung SD (wie das System sein sollte)
  \item beobachtetes Systemverhalten OBS
\end{itemize}
Ist ein Widerspruch in der Annahme, dass das System fehlerfrei funktioniert 

\subsubsection{Datenreduktion}
\begin{itemize}
  \item Kante f ist echte Teilmenge von Kante e $ \Rightarrow $ entferne e
  \item Kante e ist gleich Knoten v $ \Rightarrow $ Knoten ist in der Überdeckung
  \item Konten x hat ist nur in einer Kante mit Knoten y $ \Rightarrow $ entferne x
\end{itemize}

\end{document}






